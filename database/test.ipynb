{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的库\n",
    "%pip install rapidocr_onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip install \"unstructured[all-docs]\" -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip install pyMuPDF -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"/workspaces/LLMEasyProject/database/knowledge_db/pumpkin_book.pdf\")\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"载入后的变量类型为：{type(pages)}，\",  f\"该 PDF 一共包含 {len(pages)} 页\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[1]\n",
    "print(f\"每一个元素的类型：{type(page)}.\", \n",
    "    f\"该文档的描述性数据：{page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{page.page_content[0:100]}\", \n",
    "    sep=\"\\n------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档分割\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"/workspaces/LLMEasyProject/database/knowledge_db/pumpkin_book.pdf\")\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pages = loader.load()\n",
    "page = pages[1]\n",
    "\n",
    "# 使用递归字符文本分割器\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "text_splitter.split_text(page.page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(pages)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers\n",
    "# 文档词向量化\n",
    "# 采用huggingface的免费模型\n",
    "import os\n",
    "import openai\n",
    "import zhipuai\n",
    "import sys\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query1 = \"机器学习\"\n",
    "query2 = \"强化学习\"\n",
    "query3 = \"大语言模型\"\n",
    "\n",
    "# 通过对应的 embedding 类生成 query 的 embedding。\n",
    "emb1 = embedding.embed_query(query1)\n",
    "emb2 = embedding.embed_query(query2)\n",
    "emb3 = embedding.embed_query(query3)\n",
    "\n",
    "# 将返回结果转成 numpy 的格式，便于后续计算\n",
    "emb1 = np.array(emb1)\n",
    "emb2 = np.array(emb2)\n",
    "emb3 = np.array(emb3)\n",
    "\n",
    "\n",
    "print(f\"{query1} 生成的为长度 {len(emb1)} 的 embedding , 其前 30 个值为： {emb1[:30]}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{query1} 和 {query2} 向量之间的点积为：{np.dot(emb1, emb2)}\")\n",
    "print(f\"{query1} 和 {query3} 向量之间的点积为：{np.dot(emb1, emb3)}\")\n",
    "print(f\"{query2} 和 {query3} 向量之间的点积为：{np.dot(emb2, emb3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{query1} 和 {query2} 向量之间的余弦相似度为：{cosine_similarity(emb1.reshape(1, -1) , emb2.reshape(1, -1) )}\")\n",
    "print(f\"{query1} 和 {query3} 向量之间的余弦相似度为：{cosine_similarity(emb1.reshape(1, -1) , emb3.reshape(1, -1) )}\")\n",
    "print(f\"{query2} 和 {query3} 向量之间的余弦相似度为：{cosine_similarity(emb2.reshape(1, -1) , emb3.reshape(1, -1) )}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：400\n"
     ]
    }
   ],
   "source": [
    "# 向量数据库\n",
    "# Langchain 集成了超过 30 个不同的向量存储库。我们选择 Chroma 是因为它轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。\n",
    "\n",
    "# %pip instll pydantic-settings==2.0.1\n",
    "# %pip install chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import zhipuai\n",
    "import sys\n",
    "\n",
    "loaders_chinese = [\n",
    "    PyMuPDFLoader(\"/workspaces/LLMEasyProject/database/knowledge_db/pumpkin_book.pdf\") # 南瓜书\n",
    "    # 大家可以自行加入其他文件\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders_chinese:\n",
    "    docs.extend(loader.load())\n",
    "# 切分文档\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "\n",
    "persist_directory = '/workspaces/LLMEasyProject/database/vector_db/chroma'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs[:100], # 为了速度，只选择了前 100 个切分的 doc 进行生成。\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")\n",
    "vectordb.persist()\n",
    "\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：5\n",
      "检索到的第0个内容: \n",
      "→_→\n",
      "欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解 第 2 版》\n",
      "←_←\n",
      "第 7 章 贝叶斯分类器\n",
      "62\n",
      "7.1\n",
      "贝叶斯决策论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "62\n",
      "7.1.1\n",
      "式 (7.5) 的推导 . . . . . . . . . . . . \n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "→_→\n",
      "欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解 第 2 版》\n",
      "←_←\n",
      "第 7 章 贝叶斯分类器\n",
      "62\n",
      "7.1\n",
      "贝叶斯决策论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "62\n",
      "7.1.1\n",
      "式 (7.5) 的推导 . . . . . . . . . . . . \n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "→_→\n",
      "欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解 第 2 版》\n",
      "←_←\n",
      "第 7 章 贝叶斯分类器\n",
      "62\n",
      "7.1\n",
      "贝叶斯决策论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "62\n",
      "7.1.1\n",
      "式 (7.5) 的推导 . . . . . . . . . . . . \n",
      "--------------\n",
      "检索到的第3个内容: \n",
      "→_→\n",
      "欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解 第 2 版》\n",
      "←_←\n",
      "9.2.4\n",
      "式 (9.8) 的解释 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "99\n",
      "9.2.5\n",
      "式 (9.12) 的解释\n",
      ". . . . . . . . . . . . . . . . . . . . . \n",
      "--------------\n",
      "检索到的第4个内容: \n",
      "→_→\n",
      "欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解 第 2 版》\n",
      "←_←\n",
      "9.2.4\n",
      "式 (9.8) 的解释 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "99\n",
      "9.2.5\n",
      "式 (9.12) 的解释\n",
      ". . . . . . . . . . . . . . . . . . . . . \n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "question=\"什么是机器学习\"\n",
    "sim_docs = vectordb.similarity_search(question,k=5)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")\n",
    "\n",
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 导入检索式问答链\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# %conda install sentencepiece\n",
    "from langchain import PipelineAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "# llm = OpenAI(temperature=0)\n",
    "# 可以使用 HuggingFacePipeline 本地搭建大语言模型\n",
    "model_id = 'THUDM/chatglm2-6b-int4' # 采用 int 量化后的模型可以节省硬盘占用以及实时量化所需的运算资源\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True).half().quantize(4).cuda()\n",
    "model = model.eval()\n",
    "pipe = PipelineAI(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 声明一个检索式问答链\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "# 可以以该方式进行检索问答\n",
    "question = \"本知识库主要包含什么内容\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(f\"大语言模型的回答为：{result['result']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
